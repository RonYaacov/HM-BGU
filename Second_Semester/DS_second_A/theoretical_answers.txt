3.1 :
    in modern-day 64 bit computers w = 64
    as difined "In this task we limit the values to the long data-type of Java"
    so (a*x) is at most (2^63 -1) and at least -2^63 therfor -> (x*a) mod 2^64 = (x*a)
    the >>> operator is unsigned right shift operator and its equal to dividing by 2^(the value after >>>)
    so:  ((a · x) mod 2^64 / 2^(64-k)) = (a*x) >>> (64-k) 
    
    
3.2:
    transform any object recived to Byte-Array then transform each byte to a charcter 
    then use the Carter-Wegman hashing for strings as shown in section 3.1.1.3 and apply 
    any Carter-Wegman hashing function on the result 

3.8 and 3.10 times:
the avg time for probing with load factor of 0.5 is Inserting -- 117.03333333333333 Searching 4.312327822103327E14
the avg time for probing with load factor of 0.75 is Inserting -- 119.26666666666667 Searching 4.312386231433902E14
the avg time for probing with load factor of 0.878 is Inserting -- 71.1 Searching 4.312412322786824E14
the avg time for probing with load factor of 0.9375 is Inserting -- 74.36666666666666 Searching 4.312428533999191E14
the avg time for chaining with load factor of 0.5 is Inserting -- 374.76666666666665 Searching 4.312378405603194E14
the avg time for chaining with load factor of 0.75 is Inserting -- 301.96666666666664 Searching 4.3124356014714675E14
the avg time for chaining with load factor of 1.0 is Inserting -- 295.6666666666667 Searching 4.3124709474409744E14
the avg time for chaining with load factor of 1.5 is Inserting -- 284.9 Searching 4.3125078838640675E14
the avg time for chaining with load factor of 2.0 is Inserting -- 252.5 Searching 4.312539412082479E14
the avg time for Dietzfelbinger with load factor of 1 is Inserting -- 64.93333333333334 Searching 4.312524048355794E14
the avg time for Carter-Wegman for Strings with load factor of 1 is Inserting -- 127.16666666666667 Searching 4.312533382871127E14
3.9:
    it seams that the measurmented time is getting lower as the load factor is getting closer to 1

3.11:
    it seams that the measurmented time is getting lower as the load factor is getting closer to 1 
    but when the load factor is moving away from 1 the time slowly decend (but it is still higher then with 1) 



3.13:
    //search for val then return the node in the field next of the node that holds val
    Successor(val){
        int index <- hashFunc.hash(val);
        LinkedList<Node> list <- table[index];
        for(Node node: list){
            Pair<K,V> pair <- node.getData();
            if(pair.first() == val){
                return node.getNext().getData().second();
            }
        }
    }
    the time complexity is the same as search and it is Θ(1) expected
3.14:
    //add aditional field to the DS called Min which holds the minimum at any given time, and updates on insert (this does not change the insert time complexity)
    
    insert(K key, V value){
        Pair<K,V> pairToInsert = new Pair<K,V>(key, value); // create a new Pair = Θ(1)
        if(key < this.Min.first() && value < this.Min.second()){ // simple compare = Θ(1)
            this.Min <- pairToInsert; // Θ(1)
        }
        // the insert as described in the ChainedHashTable insert Method // the insert is Θ(1)
    }
    
    Minimum(){
        return this.Min; // return a value = Θ(1)
    } 
    in conclusion the algorithm has Θ(1) time complexity

3.15:
    // search for the val is it non-exist return 0 else return the size of the LinkedList that holds it
    Rank(val){
        int index <- hashFunc.hash(val); // the hash is a math calculation = Θ(1)
        LinkedList<Node> list <- table[index]; //Θ(1)
        for(Node node: list){// Θ(list.size)
            Pair<K,V> pair <- node.getData();
            if(pair.first() == val){
                return list.size();
            }
        }
    }
        in conclusion the algorithm has a worst case of Θ(n (the size of the worst case list)) time complexity
        and expected time complexity of Θ(1)
3.16:
    // create a counter and loop over the array of LinkedList while adding to the counter the size of each list, if after the addition the counter > index , move list.size - (counter - i) steps in the list and return the value there
    Select(i){
        if(i > capacity){ // is the index i is out of the range of the DS there is on need to loop over the all DS 
            return null;
        }
        int counter <- 0;
        for(int x=0; x < table.length; x++){ // worst case Θ(capacity) 
            List current <- table[x];
            counter += current.size;
            if(counter > i){
                int index = list.size - (counter - i);
                return list.get(index); // Θ(list.size)
            }
        }
        return null;
    }
    in conclusion the algorithm has a worst case of Θ(n)(the size of the table) time complexity